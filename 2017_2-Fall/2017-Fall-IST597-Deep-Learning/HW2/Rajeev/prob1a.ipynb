{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "'''\n",
    "Problem 1a: Softmax Regression \\& the XOR Problem\n",
    "\n",
    "@author - Min-Chun Wu\n",
    "'''\n",
    "\n",
    "def computeNumGrad(X,y,theta,reg): # returns approximate nabla\n",
    "# WRITEME: write your code here to complete the routine\n",
    "    eps = 1e-4\n",
    "    nabla_n = []\n",
    "# NOTE: you do not have to use any of the code here in your implementation...\n",
    "    for i in range(len(theta)):\n",
    "        param = theta[i]\n",
    "        param_grad = np.zeros(param.shape)\n",
    "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "        # Initializing the parameters at (x+eps) and (x-eps)\n",
    "            theta_plus_eps = theta\n",
    "            theta_minus_eps = theta\n",
    "            ix = it.multi_index\n",
    "        # Evaluate function at x+eps i.e f(x+eps)\n",
    "            theta_plus_eps[i][ix] = param[ix] + eps\n",
    "            f_x_plus_eps = computeCost(X,y,theta_plus_eps,reg)\n",
    "        # Reset theta\n",
    "            theta[i][ix] = param[ix] - eps        \n",
    "        # Evaluate function at x i.e f(x-eps)\n",
    "            theta_minus_eps[i][ix] = param[ix] - eps\n",
    "            f_x_minus_eps = computeCost(X,y,theta_minus_eps,reg)\n",
    "        # Reset theta\n",
    "            theta[i][ix] = param[ix] + eps\n",
    "        # Finally gradient at x\n",
    "            param_grad[ix] = (f_x_plus_eps - f_x_minus_eps)/(2*eps)\n",
    "        # Iterating over all dimensions\n",
    "            it.iternext()\n",
    "        nabla_n.append(param_grad)\n",
    "    return tuple(nabla_n)\n",
    "\n",
    "def softmax_loss(X, y):\n",
    "# Forward pass\n",
    "    N = X.shape[0]\n",
    "#    X -= np.max(X, axis=1, keepdims=True)\n",
    "    exp_vals = np.exp(X)\n",
    "    probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "    loss = -np.mean(np.log(probs[range(N), y]))\n",
    "# Backward pass\n",
    "    dX = np.array(probs, copy=True)\n",
    "    dX[range(N), y] -= 1\n",
    "    dX /= N\n",
    "    return loss, probs, dX\n",
    "\n",
    "def computeGrad(X,y,theta,reg): # returns nabla\n",
    "# WRITEME: write your code here to complete the routine\n",
    "    W, b = theta[0], theta[1]\n",
    "    f = X.dot(W) + b\n",
    "    df = softmax_loss(f,y)[2]\n",
    "    dW = np.dot(X.T, df) + reg * W\n",
    "    db = np.sum(df, axis=0)\n",
    "    return (dW,db)\n",
    "\n",
    "def computeCost(X,y,theta,reg):\n",
    "# WRITEME: write your code here to complete the routine\n",
    "    W, b = theta[0], theta[1]\n",
    "    N = X.shape[0]\n",
    "    f = X.dot(W) + b\n",
    "    data_loss = softmax_loss(f,y)[0]\n",
    "    reg_loss = 0.5 * reg * np.sum(W**2)\n",
    "    cost = data_loss + reg_loss\n",
    "    return cost\n",
    "\n",
    "def predict(X,theta):\n",
    "# WRITEME: write your code here to complete the routine\n",
    "    W, b = theta[0], theta[1]\n",
    "# evaluate class scores\n",
    "    scores = X.dot(W) + b\n",
    "    probs = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    return scores, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param 0 is CORRECT, error = 3.98782281694e-10\n",
      "Param 1 is CORRECT, error = 3.72447015252e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Load in the data from disk\n",
    "path = os.getcwd() + '/data/xor.dat'  \n",
    "data = pd.read_csv(path, header=None) \n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] \n",
    "\n",
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)\n",
    "y = y.flatten()\n",
    "\n",
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "D = X.shape[1]\n",
    "K = np.amax(y) + 1\n",
    "\n",
    "# initialize parameters in such a way to play nicely with the gradient-check!\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K)) + 1.0\n",
    "theta = (W,b)\n",
    "\n",
    "# some hyperparameters\n",
    "reg = 1e-3 # regularization strength\n",
    "nabla_n = computeNumGrad(X,y,theta,reg)\n",
    "nabla = computeGrad(X,y,theta,reg)\n",
    "nabla_n = list(nabla_n)\n",
    "nabla = list(nabla)\n",
    "\n",
    "for jj in range(0,len(nabla)):\n",
    "    is_incorrect = 0 # set to false\n",
    "    grad = nabla[jj]\n",
    "    grad_n = nabla_n[jj]\n",
    "    err = np.linalg.norm(grad_n - grad) / (np.linalg.norm(grad_n + grad))\n",
    "    if(err > 1e-7):\n",
    "        print(\"Param {0} is WRONG, error = {1}\".format(jj, err))\n",
    "    else:\n",
    "        print(\"Param {0} is CORRECT, error = {1}\".format(jj, err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.693252\n",
      "iteration 10: loss 0.693147\n",
      "iteration 20: loss 0.693147\n",
      "iteration 30: loss 0.693147\n",
      "iteration 40: loss 0.693147\n",
      "iteration 50: loss 0.693147\n",
      "iteration 60: loss 0.693147\n",
      "iteration 70: loss 0.693147\n",
      "iteration 80: loss 0.693147\n",
      "iteration 90: loss 0.693147\n",
      "training accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize parameters for generic training\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "theta = (W,b)\n",
    "\n",
    "no_epochs = 100\n",
    "check = 10 # every so many pass/epochs, print loss/error to terminal\n",
    "step_size = 2\n",
    "reg = 1e-1 # regularization strength\n",
    "\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in xrange(no_epochs):\n",
    "# WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "    theta = (W, b)\n",
    "    loss = computeCost(X,y,theta,reg)\n",
    "    if i % check == 0:\n",
    "        print \"iteration %d: loss %f\" % (i, loss)\n",
    "\n",
    "    # perform a parameter update\n",
    "    # WRITEME: write your update rule(s) here\n",
    "    dW, db = computeGrad(X,y,theta,reg)\n",
    "    W = W - step_size*dW\n",
    "    b = b - step_size*db\n",
    " \n",
    "# TODO: remove this line below once you have correctly implemented/gradient-checked your various sub-routines\n",
    "#sys.exit(0)\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores, probs = predict(X,theta)\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print 'training accuracy: %.2f%%' % (100*np.mean(predicted_class == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
