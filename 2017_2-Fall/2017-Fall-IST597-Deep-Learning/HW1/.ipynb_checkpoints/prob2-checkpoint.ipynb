{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: you will need to tinker with the meta-parameters below yourself (do not think of them as defaults by any means)\n",
    "# meta-parameters for program\n",
    "trial_name = 'p1_fit' # will add a unique sub-string to output of this program\n",
    "degree = 3 # p, order of model\n",
    "beta = 3.0 # regularization coefficient\n",
    "alpha = 0.5 # step size coefficient\n",
    "eps = 0.0 # controls convergence criterion\n",
    "n_epoch = 100 # number of epochs (full passes through the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data/prob2.dat'  \n",
    "data = pd.read_csv(path, header=None, names=['X', 'Y']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] \n",
    "\n",
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)\n",
    "y = np.array(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply feature map to input features x1\n",
    "# WRITEME: write code to turn X_feat into a polynomial feature map (hint: you could use a loop and array concatenation)\n",
    "X_feat = np.ones((X.shape[0], 1))\n",
    "for j in range(1,degree+1):\n",
    "    X_j = np.array([x**j for x in X])\n",
    "    X_feat = np.concatenate((X_feat, X_j), axis = 1)\n",
    "X_feat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to numpy arrays and initalize the parameter array theta \n",
    "w = np.zeros((1,X_feat.shape[1]-1))\n",
    "b = np.array([0])\n",
    "theta = (b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) The standard * is the entrywise product. <br>\n",
    "(2) To perform matrix multiplication, use np.dot( , )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 16],\n",
       "       [29, 36]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[3,4],[5,6]])\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta, beta): # loss is now Bernoulli cross-entropy/log likelihood\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    m = X.shape[0]\n",
    "    THETA = np.concatenate((theta[0].reshape(1,1),theta[1].reshape(degree,1)),axis = 0)\n",
    "    Cost = (1/(2*m))*((np.dot(X_feat,THETA)-y)**2).sum()+beta/2*(theta[1]**2).sum()\n",
    "    return Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGrad(X, y, theta, beta):\n",
    "    # WRITEME: write your code here to complete the routine (\n",
    "    # NOTE: you do not have to use the partial derivative symbols below, they are there to guide your thinking)\n",
    "    m = X.shape[0]\n",
    "    THETA = np.concatenate((theta[0].reshape(1,1),theta[1].reshape(degree,1)),axis = 0)\n",
    "    dL_db = (1/m)*(np.dot(X_feat,THETA)-y).sum() # derivative w.r.t. model bias b\n",
    "    dL_dw = np.array([(1/m)*(((np.dot(X_feat,THETA)-y)*X_feat[:,i].reshape(m,1)).sum())+(beta*theta[1])[0,i-1] for i in range(1,theta[1].shape[1]+1)]) # derivative w.r.t model weights w\n",
    "    nabla = (dL_db, dL_dw) # nabla represents the full gradient\n",
    "    return nabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 L = 0.2913055978424858\n",
      " 0 L = 0.28585912152057374\n",
      " 1 L = 0.283918919957383\n",
      " 2 L = 0.28316228825511397\n",
      " 3 L = 0.28284258189278455\n",
      " 4 L = 0.28269901805368924\n",
      " 5 L = 0.28263185820319514\n",
      " 6 L = 0.28259963499029916\n",
      " 7 L = 0.2825839428422267\n",
      " 8 L = 0.2825762361154492\n",
      " 9 L = 0.28257243323017384\n",
      " 10 L = 0.28257055176389007\n",
      " 11 L = 0.2825696195636105\n",
      " 12 L = 0.2825691573216849\n",
      " 13 L = 0.2825689280126293\n",
      " 14 L = 0.28256881422915076\n",
      " 15 L = 0.28256875776196355\n",
      " 16 L = 0.28256872973693875\n",
      " 17 L = 0.28256871582734844\n",
      " 18 L = 0.28256870892347\n",
      " 19 L = 0.28256870549675556\n",
      " 20 L = 0.28256870379590504\n",
      " 21 L = 0.28256870295168346\n",
      " 22 L = 0.2825687025326508\n",
      " 23 L = 0.2825687023246619\n",
      " 24 L = 0.2825687022214255\n",
      " 25 L = 0.2825687021701836\n",
      " 26 L = 0.2825687021447493\n",
      " 27 L = 0.28256870213212487\n",
      " 28 L = 0.28256870212585866\n",
      " 29 L = 0.28256870212274837\n",
      " 30 L = 0.28256870212120455\n",
      " 31 L = 0.28256870212043833\n",
      " 32 L = 0.28256870212005797\n",
      " 33 L = 0.2825687021198691\n",
      " 34 L = 0.2825687021197754\n",
      " 35 L = 0.28256870211972895\n",
      " 36 L = 0.28256870211970586\n",
      " 37 L = 0.2825687021196944\n",
      " 38 L = 0.2825687021196887\n",
      " 39 L = 0.2825687021196859\n",
      " 40 L = 0.28256870211968454\n",
      " 41 L = 0.2825687021196838\n",
      " 42 L = 0.2825687021196835\n",
      " 43 L = 0.2825687021196833\n",
      " 44 L = 0.2825687021196832\n",
      " 45 L = 0.28256870211968316\n",
      " 46 L = 0.28256870211968316\n",
      " 47 L = 0.2825687021196831\n",
      " 48 L = 0.28256870211968316\n",
      " 49 L = 0.2825687021196831\n",
      " 50 L = 0.2825687021196831\n",
      " 51 L = 0.28256870211968316\n",
      " 52 L = 0.28256870211968316\n",
      " 53 L = 0.2825687021196831\n",
      " 54 L = 0.2825687021196831\n",
      " 55 L = 0.28256870211968305\n",
      " 56 L = 0.2825687021196831\n",
      " 57 L = 0.28256870211968316\n",
      " 58 L = 0.28256870211968316\n",
      " 59 L = 0.2825687021196831\n",
      " 60 L = 0.28256870211968316\n",
      " 61 L = 0.28256870211968316\n",
      " 62 L = 0.28256870211968316\n",
      " 63 L = 0.2825687021196831\n",
      " 64 L = 0.2825687021196831\n",
      " 65 L = 0.2825687021196831\n",
      " 66 L = 0.2825687021196831\n",
      " 67 L = 0.2825687021196831\n",
      " 68 L = 0.2825687021196831\n",
      " 69 L = 0.2825687021196831\n",
      " 70 L = 0.2825687021196831\n",
      " 71 L = 0.2825687021196831\n",
      " 72 L = 0.2825687021196831\n",
      " 73 L = 0.28256870211968316\n",
      " 74 L = 0.2825687021196831\n",
      " 75 L = 0.2825687021196831\n",
      " 76 L = 0.2825687021196831\n",
      " 77 L = 0.2825687021196831\n",
      " 78 L = 0.2825687021196831\n",
      " 79 L = 0.2825687021196831\n",
      " 80 L = 0.2825687021196831\n",
      " 81 L = 0.2825687021196831\n",
      " 82 L = 0.2825687021196831\n",
      " 83 L = 0.2825687021196831\n",
      " 84 L = 0.2825687021196831\n",
      " 85 L = 0.2825687021196831\n",
      " 86 L = 0.28256870211968316\n",
      " 87 L = 0.2825687021196831\n",
      " 88 L = 0.2825687021196831\n",
      " 89 L = 0.2825687021196831\n",
      " 90 L = 0.28256870211968316\n",
      " 91 L = 0.28256870211968316\n",
      " 92 L = 0.2825687021196831\n",
      " 93 L = 0.2825687021196831\n",
      " 94 L = 0.2825687021196831\n",
      " 95 L = 0.28256870211968316\n",
      " 96 L = 0.2825687021196831\n",
      " 97 L = 0.2825687021196831\n",
      " 98 L = 0.2825687021196831\n",
      " 99 L = 0.2825687021196831\n",
      "w =  [[-0.03142197 -0.00529071 -0.03797536]]\n",
      "b =  [ 0.09466877]\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy arrays and initalize the parameter array theta \n",
    "w = np.zeros((1,X_feat.shape[1]-1))\n",
    "b = np.array([0])\n",
    "theta = (b, w)\n",
    "\n",
    "L = computeCost(X, y, theta, beta)\n",
    "print(\"-1 L = {0}\".format(L))\n",
    "i = 0\n",
    "\n",
    "while(i < n_epoch):\n",
    "    dL_db, dL_dw = computeGrad(X, y, theta, beta)\n",
    "    b = theta[0]\n",
    "    w = theta[1]\n",
    "    # update rules go here...\n",
    "    # WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "    b = b - alpha*dL_db\n",
    "    w = w - alpha*dL_dw\n",
    "    theta = (b,w)\n",
    "    \n",
    "    L = computeCost(X, y, theta, beta)\n",
    "    \n",
    "    # WRITEME: write code to perform a check for convergence (or simply to halt early)\n",
    "    \n",
    "    print(\" {0} L = {1}\".format(i,L))\n",
    "    i += 1\n",
    "# print parameter values found after the search\n",
    "print(\"w = \",w)\n",
    "print(\"b = \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -9.73379901e-01,   9.47468431e-01,\n",
       "         -9.22246727e-01],\n",
       "       [  1.00000000e+00,  -9.53671495e-01,   9.09489320e-01,\n",
       "         -8.67354039e-01],\n",
       "       [  1.00000000e+00,  -9.33963089e-01,   8.72287051e-01,\n",
       "         -8.14683908e-01],\n",
       "       [  1.00000000e+00,  -9.14254683e-01,   8.35861625e-01,\n",
       "         -7.64190404e-01],\n",
       "       [  1.00000000e+00,  -8.94546277e-01,   8.00213041e-01,\n",
       "         -7.15827596e-01],\n",
       "       [  1.00000000e+00,  -8.74837871e-01,   7.65341300e-01,\n",
       "         -6.69549553e-01],\n",
       "       [  1.00000000e+00,  -8.55129464e-01,   7.31246401e-01,\n",
       "         -6.25310343e-01],\n",
       "       [  1.00000000e+00,  -8.35421058e-01,   6.97928345e-01,\n",
       "         -5.83064037e-01],\n",
       "       [  1.00000000e+00,  -8.15712652e-01,   6.65387131e-01,\n",
       "         -5.42764702e-01],\n",
       "       [  1.00000000e+00,  -7.96004246e-01,   6.33622760e-01,\n",
       "         -5.04366408e-01],\n",
       "       [  1.00000000e+00,  -7.76295840e-01,   6.02635232e-01,\n",
       "         -4.67823224e-01],\n",
       "       [  1.00000000e+00,  -7.56587434e-01,   5.72424546e-01,\n",
       "         -4.33089218e-01],\n",
       "       [  1.00000000e+00,  -7.36879028e-01,   5.42990702e-01,\n",
       "         -4.00118461e-01],\n",
       "       [  1.00000000e+00,  -7.17170622e-01,   5.14333701e-01,\n",
       "         -3.68865021e-01],\n",
       "       [  1.00000000e+00,  -6.97462216e-01,   4.86453543e-01,\n",
       "         -3.39282966e-01],\n",
       "       [  1.00000000e+00,  -6.77753810e-01,   4.59350227e-01,\n",
       "         -3.11326367e-01],\n",
       "       [  1.00000000e+00,  -6.58045404e-01,   4.33023754e-01,\n",
       "         -2.84949291e-01],\n",
       "       [  1.00000000e+00,  -6.38336998e-01,   4.07474123e-01,\n",
       "         -2.60105809e-01],\n",
       "       [  1.00000000e+00,  -6.18628592e-01,   3.82701335e-01,\n",
       "         -2.36749988e-01],\n",
       "       [  1.00000000e+00,  -5.98920186e-01,   3.58705389e-01,\n",
       "         -2.14835898e-01],\n",
       "       [  1.00000000e+00,  -5.79211780e-01,   3.35486286e-01,\n",
       "         -1.94317609e-01],\n",
       "       [  1.00000000e+00,  -5.59503374e-01,   3.13044025e-01,\n",
       "         -1.75149188e-01],\n",
       "       [  1.00000000e+00,  -5.39794968e-01,   2.91378607e-01,\n",
       "         -1.57284706e-01],\n",
       "       [  1.00000000e+00,  -5.20086562e-01,   2.70490032e-01,\n",
       "         -1.40678231e-01],\n",
       "       [  1.00000000e+00,  -5.00378156e-01,   2.50378299e-01,\n",
       "         -1.25283831e-01],\n",
       "       [  1.00000000e+00,  -4.80669750e-01,   2.31043408e-01,\n",
       "         -1.11055577e-01],\n",
       "       [  1.00000000e+00,  -4.60961344e-01,   2.12485360e-01,\n",
       "         -9.79475372e-02],\n",
       "       [  1.00000000e+00,  -4.41252938e-01,   1.94704155e-01,\n",
       "         -8.59137804e-02],\n",
       "       [  1.00000000e+00,  -4.21544532e-01,   1.77699792e-01,\n",
       "         -7.49083757e-02],\n",
       "       [  1.00000000e+00,  -4.01836126e-01,   1.61472272e-01,\n",
       "         -6.48853921e-02],\n",
       "       [  1.00000000e+00,  -3.82127720e-01,   1.46021594e-01,\n",
       "         -5.57988987e-02],\n",
       "       [  1.00000000e+00,  -3.62419314e-01,   1.31347759e-01,\n",
       "         -4.76029646e-02],\n",
       "       [  1.00000000e+00,  -3.42710907e-01,   1.17450766e-01,\n",
       "         -4.02516586e-02],\n",
       "       [  1.00000000e+00,  -3.23002501e-01,   1.04330616e-01,\n",
       "         -3.36990499e-02],\n",
       "       [  1.00000000e+00,  -3.03294095e-01,   9.19873083e-02,\n",
       "         -2.78992075e-02],\n",
       "       [  1.00000000e+00,  -2.83585689e-01,   8.04208432e-02,\n",
       "         -2.28062003e-02],\n",
       "       [  1.00000000e+00,  -2.63877283e-01,   6.96312206e-02,\n",
       "         -1.83740973e-02],\n",
       "       [  1.00000000e+00,  -2.44168877e-01,   5.96184406e-02,\n",
       "         -1.45569677e-02],\n",
       "       [  1.00000000e+00,  -2.24460471e-01,   5.03825031e-02,\n",
       "         -1.13088804e-02],\n",
       "       [  1.00000000e+00,  -2.04752065e-01,   4.19234082e-02,\n",
       "         -8.58390441e-03],\n",
       "       [  1.00000000e+00,  -1.85043659e-01,   3.42411558e-02,\n",
       "         -6.33610876e-03],\n",
       "       [  1.00000000e+00,  -1.65335253e-01,   2.73357459e-02,\n",
       "         -4.51956247e-03],\n",
       "       [  1.00000000e+00,  -1.45626847e-01,   2.12071786e-02,\n",
       "         -3.08833455e-03],\n",
       "       [  1.00000000e+00,  -1.25918441e-01,   1.58554538e-02,\n",
       "         -1.99649402e-03],\n",
       "       [  1.00000000e+00,  -1.06210035e-01,   1.12805715e-02,\n",
       "         -1.19810990e-03],\n",
       "       [  1.00000000e+00,  -8.65016290e-02,   7.48253181e-03,\n",
       "         -6.47251191e-04],\n",
       "       [  1.00000000e+00,  -6.67932229e-02,   4.46133463e-03,\n",
       "         -2.97986918e-04],\n",
       "       [  1.00000000e+00,  -4.70848169e-02,   2.21697998e-03,\n",
       "         -1.04386096e-04],\n",
       "       [  1.00000000e+00,  -2.73764108e-02,   7.49467871e-04,\n",
       "         -2.05177403e-05],\n",
       "       [  1.00000000e+00,  -7.66800481e-03,   5.87982977e-05,\n",
       "         -4.50865630e-07],\n",
       "       [  1.00000000e+00,   1.20404012e-02,   1.44971262e-04,\n",
       "          1.74551216e-06],\n",
       "       [  1.00000000e+00,   3.17488073e-02,   1.00798676e-03,\n",
       "          3.20023775e-05],\n",
       "       [  1.00000000e+00,   5.14572133e-02,   2.64784480e-03,\n",
       "          1.36250715e-04],\n",
       "       [  1.00000000e+00,   7.11656193e-02,   5.06454538e-03,\n",
       "          3.60421508e-04],\n",
       "       [  1.00000000e+00,   9.08740254e-02,   8.25808849e-03,\n",
       "          7.50445743e-04],\n",
       "       [  1.00000000e+00,   1.10582431e-01,   1.22284741e-02,\n",
       "          1.35225440e-03],\n",
       "       [  1.00000000e+00,   1.30290837e-01,   1.69757023e-02,\n",
       "          2.21177847e-03],\n",
       "       [  1.00000000e+00,   1.49999244e-01,   2.24997731e-02,\n",
       "          3.37494894e-03],\n",
       "       [  1.00000000e+00,   1.69707650e-01,   2.88006863e-02,\n",
       "          4.88769678e-03],\n",
       "       [  1.00000000e+00,   1.89416056e-01,   3.58784421e-02,\n",
       "          6.79595299e-03],\n",
       "       [  1.00000000e+00,   2.09124462e-01,   4.37330404e-02,\n",
       "          9.14564854e-03],\n",
       "       [  1.00000000e+00,   2.28832868e-01,   5.23644813e-02,\n",
       "          1.19827144e-02],\n",
       "       [  1.00000000e+00,   2.48541274e-01,   6.17727647e-02,\n",
       "          1.53530816e-02],\n",
       "       [  1.00000000e+00,   2.68249680e-01,   7.19578907e-02,\n",
       "          1.93026811e-02],\n",
       "       [  1.00000000e+00,   2.87958086e-01,   8.29198592e-02,\n",
       "          2.38774439e-02],\n",
       "       [  1.00000000e+00,   3.07666492e-01,   9.46586702e-02,\n",
       "          2.91233010e-02],\n",
       "       [  1.00000000e+00,   3.27374898e-01,   1.07174324e-01,\n",
       "          3.50861833e-02],\n",
       "       [  1.00000000e+00,   3.47083304e-01,   1.20466820e-01,\n",
       "          4.18120218e-02],\n",
       "       [  1.00000000e+00,   3.66791710e-01,   1.34536158e-01,\n",
       "          4.93467476e-02],\n",
       "       [  1.00000000e+00,   3.86500116e-01,   1.49382340e-01,\n",
       "          5.77362916e-02],\n",
       "       [  1.00000000e+00,   4.06208522e-01,   1.65005363e-01,\n",
       "          6.70265848e-02],\n",
       "       [  1.00000000e+00,   4.25916928e-01,   1.81405230e-01,\n",
       "          7.72635581e-02],\n",
       "       [  1.00000000e+00,   4.45625334e-01,   1.98581938e-01,\n",
       "          8.84931426e-02],\n",
       "       [  1.00000000e+00,   4.65333740e-01,   2.16535490e-01,\n",
       "          1.00761269e-01],\n",
       "       [  1.00000000e+00,   4.85042146e-01,   2.35265884e-01,\n",
       "          1.14113869e-01],\n",
       "       [  1.00000000e+00,   5.04750552e-01,   2.54773120e-01,\n",
       "          1.28596873e-01],\n",
       "       [  1.00000000e+00,   5.24458958e-01,   2.75057199e-01,\n",
       "          1.44256212e-01],\n",
       "       [  1.00000000e+00,   5.44167364e-01,   2.96118120e-01,\n",
       "          1.61137817e-01],\n",
       "       [  1.00000000e+00,   5.63875770e-01,   3.17955884e-01,\n",
       "          1.79287619e-01],\n",
       "       [  1.00000000e+00,   5.83584176e-01,   3.40570491e-01,\n",
       "          1.98751549e-01],\n",
       "       [  1.00000000e+00,   6.03292582e-01,   3.63961940e-01,\n",
       "          2.19575539e-01],\n",
       "       [  1.00000000e+00,   6.23000988e-01,   3.88130232e-01,\n",
       "          2.41805518e-01],\n",
       "       [  1.00000000e+00,   6.42709394e-01,   4.13075366e-01,\n",
       "          2.65487418e-01],\n",
       "       [  1.00000000e+00,   6.62417801e-01,   4.38797342e-01,\n",
       "          2.90667170e-01],\n",
       "       [  1.00000000e+00,   6.82126207e-01,   4.65296162e-01,\n",
       "          3.17390706e-01],\n",
       "       [  1.00000000e+00,   7.01834613e-01,   4.92571823e-01,\n",
       "          3.45703955e-01],\n",
       "       [  1.00000000e+00,   7.21543019e-01,   5.20624328e-01,\n",
       "          3.75652849e-01],\n",
       "       [  1.00000000e+00,   7.41251425e-01,   5.49453675e-01,\n",
       "          4.07283319e-01],\n",
       "       [  1.00000000e+00,   7.60959831e-01,   5.79059864e-01,\n",
       "          4.40641296e-01],\n",
       "       [  1.00000000e+00,   7.80668237e-01,   6.09442896e-01,\n",
       "          4.75772711e-01],\n",
       "       [  1.00000000e+00,   8.00376643e-01,   6.40602770e-01,\n",
       "          5.12723495e-01],\n",
       "       [  1.00000000e+00,   8.20085049e-01,   6.72539487e-01,\n",
       "          5.51539578e-01],\n",
       "       [  1.00000000e+00,   8.39793455e-01,   7.05253047e-01,\n",
       "          5.92266893e-01],\n",
       "       [  1.00000000e+00,   8.59501861e-01,   7.38743449e-01,\n",
       "          6.34951369e-01],\n",
       "       [  1.00000000e+00,   8.79210267e-01,   7.73010694e-01,\n",
       "          6.79638938e-01],\n",
       "       [  1.00000000e+00,   8.98918673e-01,   8.08054781e-01,\n",
       "          7.26375531e-01],\n",
       "       [  1.00000000e+00,   9.18627079e-01,   8.43875710e-01,\n",
       "          7.75207079e-01],\n",
       "       [  1.00000000e+00,   9.38335485e-01,   8.80473483e-01,\n",
       "          8.26179512e-01],\n",
       "       [  1.00000000e+00,   9.58043891e-01,   9.17848097e-01,\n",
       "          8.79338763e-01],\n",
       "       [  1.00000000e+00,   9.77752297e-01,   9.55999555e-01,\n",
       "          9.34730761e-01]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.linspace(data.X.min(), data.X.max(), 100).reshape(100,1)\n",
    "\n",
    "# apply feature map to input features x1\n",
    "# WRITEME: write code to turn X_test into a polynomial feature map (hint: you could use a loop and array concatenation)\n",
    "X_test_feat = np.ones((X_test.shape[0], 1))\n",
    "for j in range(1,degree+1):\n",
    "    X_test_j = np.array([x**j for x in X_test])\n",
    "    X_test_feat = np.concatenate((X_test_feat, X_test_j), axis = 1)\n",
    "\n",
    "X_test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09466877]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[0].reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = np.concatenate((theta[0].reshape(1,1), theta[1]), axis = 1).T\n",
    "np.dot(X_test_feat,Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 L = 0.2913055978424858\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-3d86bbef5f05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdL_db\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdL_dw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "'''\n",
    "IST 597: Foundations of Deep Learning\n",
    "Problem 2: Polynomial Regression & \n",
    "\n",
    "@author - Alexander G. Ororbia II\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "'''\n",
    "\n",
    "# NOTE: you will need to tinker with the meta-parameters below yourself (do not think of them as defaults by any means)\n",
    "# meta-parameters for program\n",
    "trial_name = 'p1_fit' # will add a unique sub-string to output of this program\n",
    "degree = 1 # p, order of model\n",
    "beta = 0.0 # regularization coefficient\n",
    "alpha = 0.0 # step size coefficient\n",
    "eps = 0.0 # controls convergence criterion\n",
    "n_epoch = 1 # number of epochs (full passes through the dataset)\n",
    "\n",
    "# begin simulation\n",
    "\n",
    "def regress(X, theta):\n",
    "    Theta = np.concatenate((theta[0].reshape(1,1), theta[1]), axis = 1).T\n",
    "    return np.dot(X,Theta)\n",
    "\n",
    "def gaussian_log_likelihood(mu, y):\n",
    "\t# WRITEME: write your code here to complete the routine\n",
    "\treturn -1.0\n",
    "\t\n",
    "def computeCost(X, y, theta, beta): # loss is now Bernoulli cross-entropy/log likelihood\n",
    "    # WRITEME: write your code here to complete the routine\n",
    "    m = X.shape[0]\n",
    "    THETA = np.concatenate((theta[0].reshape(1,1),theta[1].reshape(degree,1)),axis = 0)\n",
    "    Cost = (1/(2*m))*((np.dot(X_feat,THETA)-y)**2).sum()+beta/2*(theta[1]**2).sum()\n",
    "    return Cost\n",
    "\n",
    "def computeGrad(X, y, theta, beta):\n",
    "    # WRITEME: write your code here to complete the routine (\n",
    "    # NOTE: you do not have to use the partial derivative symbols below, they are there to guide your thinking)\n",
    "    m = X.shape[0]\n",
    "    THETA = np.concatenate((theta[0].reshape(1,1),theta[1].reshape(degree,1)),axis = 0)\n",
    "    dL_db = (1/m)*(np.dot(X_feat,THETA)-y).sum() # derivative w.r.t. model bias b\n",
    "    dL_dw = np.array([(1/m)*(((np.dot(X_feat,THETA)-y)*X_feat[:,i].reshape(m,1)).sum())+(beta*theta[1])[0,i-1] for i in range(1,theta[1].shape[1]+1)]) # derivative w.r.t model weights w\n",
    "    nabla = (dL_db, dL_dw) # nabla represents the full gradient\n",
    "    return nabla\n",
    "\n",
    "path = os.getcwd() + '/data/prob2.dat'  \n",
    "data = pd.read_csv(path, header=None, names=['X', 'Y']) \n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] \n",
    "\n",
    "# convert from data frames to numpy matrices\n",
    "X = np.array(X.values)\n",
    "y = np.array(y.values)\n",
    "\n",
    "# apply feature map to input features x1\n",
    "# WRITEME: write code to turn X_feat into a polynomial feature map (hint: you could use a loop and array concatenation)\n",
    "X_feat = np.ones((X.shape[0], 1))\n",
    "for j in range(1,degree+1):\n",
    "    X_j = np.array([x**j for x in X])\n",
    "    X_feat = np.concatenate((X_feat, X_j), axis = 1)\n",
    "\n",
    "X_feat\n",
    "\n",
    "# convert to numpy arrays and initalize the parameter array theta \n",
    "w = np.zeros((1,X_feat.shape[1]-1))\n",
    "b = np.array([0])\n",
    "theta = (b, w)\n",
    "\n",
    "L = computeCost(X, y, theta, beta)\n",
    "print(\"-1 L = {0}\".format(L))\n",
    "i = 0\n",
    "\n",
    "while(i < n_epoch):\n",
    "    dL_db, dL_dw = computeGrad(X, y, theta, beta)\n",
    "    b = theta[0]\n",
    "    w = theta[1]\n",
    "    # update rules go here...\n",
    "    # WRITEME: write your code here to perform a step of gradient descent & record anything else desired for later\n",
    "    b = b - alpha*dL_db\n",
    "    w = w - alpha*dL_dw\n",
    "    theta = (b,w)\n",
    "    \n",
    "    L = computeCost(X, y, theta, beta)\n",
    "    \n",
    "    # WRITEME: write code to perform a check for convergence (or simply to halt early)\n",
    "    \n",
    "    print(\" {0} L = {1}\".format(i,L))\n",
    "    i += 1\n",
    "# print parameter values found after the search\n",
    "print(\"w = \",w)\n",
    "print(\"b = \",b)\n",
    "\n",
    "kludge = 0.25\n",
    "# visualize the fit against the data\n",
    "X_test = np.linspace(data.X.min(), data.X.max(), 100).reshape(100,1)\n",
    "\n",
    "# apply feature map to input features x1\n",
    "# WRITEME: write code to turn X_test into a polynomial feature map (hint: you could use a loop and array concatenation)\n",
    "X_test_feat = np.ones((X_test.shape[0], 1))\n",
    "for j in range(1,degree+1):\n",
    "    X_test_j = np.array([x**j for x in X_test])\n",
    "    X_test_feat = np.concatenate((X_test_feat, X_test_j), axis = 1)\n",
    "\n",
    "X_test_feat\n",
    "\n",
    "plt.plot(X_test, regress(X_test_feat, theta), label=\"Model\")\n",
    "plt.scatter(X[:,0], y, edgecolor='g', s=20, label=\"Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((np.amin(X_test) - kludge, np.amax(X_test) + kludge))\n",
    "plt.ylim((np.amin(y) - kludge, np.amax(y) + kludge))\n",
    "plt.legend(loc=\"best\")\n",
    "# WRITEME: write your code here to save plot to disk (look up documentation/inter-webs for matplotlib)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
